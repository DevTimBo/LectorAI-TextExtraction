{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea602cf0",
   "metadata": {},
   "source": [
    "# Author: Tim Harmling and Alexej Kravtschenko\n",
    "- **Note:** This notebook was written by the combined effort of Tim Harmling and Alexej Kravtschenko \n",
    "- **Description:** With this notebook you can train your own handwriting model form scratch or use an existing model for transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1502b4cfa6e7903",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c81f63399a8a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:10:52.014719400Z",
     "start_time": "2024-02-15T16:10:45.271935100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unsere Klassen\n",
    "import load_data\n",
    "import load_transfer_data\n",
    "import models\n",
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442b33bf1c3d535",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90013526b312f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:10:52.040754600Z",
     "start_time": "2024-02-15T16:10:52.005999700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76747d5c3df28a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d944e31ec1a2605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:10:52.562893800Z",
     "start_time": "2024-02-15T16:10:52.051267500Z"
    }
   },
   "outputs": [],
   "source": [
    "load_data.print_samples(IAM_DATASET_PATH)\n",
    "x_train_img_paths, y_train_labels = load_data.get_train_data()\n",
    "x_test_img_paths, y_test_labels = load_data.get_test_data()\n",
    "x_val_img_paths, y_val_labels = load_data.get_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2b7995883e4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:10:52.602813Z",
     "start_time": "2024-02-15T16:10:52.581854900Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training path: {x_train_img_paths[0:2]}\", y_train_labels[0:2])\n",
    "print(f\"Validation path: {x_val_img_paths[0:2]}\", y_val_labels[0:2])\n",
    "print(f\"Testing path: {x_test_img_paths[0:2]}\", y_test_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab75f6db361259",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a385c9dcb30be19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:10:53.655051600Z",
     "start_time": "2024-02-15T16:10:52.597323400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Has to be here because load data functions need to be called before\n",
    "import tokenizer\n",
    "\n",
    "train_ds = tokenizer.prepare_dataset(x_train_img_paths, y_train_labels, (IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "val_ds = tokenizer.prepare_dataset(x_val_img_paths, y_val_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "test_ds = tokenizer.prepare_dataset(x_test_img_paths, y_test_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "aug_train_ds = tokenizer.prepare_augmented_dataset(x_train_img_paths, y_train_labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320fbb2c02999f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Show Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dcf82c6677caa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:11:33.595403400Z",
     "start_time": "2024-02-15T16:10:53.659578800Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in aug_train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "    ax = plt.subplots(4, 4, figsize=(32, 8))[1]\n",
    "\n",
    "    for i in range(min(16,BATCH_SIZE)):\n",
    "        img = images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        # Gather indices where label!= padding_token.\n",
    "        label = labels[i]\n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, tokenizer.padding_token)))\n",
    "        # Convert to string.\n",
    "        label = tf.strings.reduce_join(tokenizer.num_to_char(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b04141fedb9fc1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8272653bffdfec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:11:35.740549400Z",
     "start_time": "2024-02-15T16:11:35.654168300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_keras_string =\"_weights.keras\"\n",
    "\n",
    "def model_load_weights_if_exists(model):\n",
    "    \"\"\"Loads model weights if they exist.\n",
    "\n",
    "    This function checks if the model weights exist and loads them into the model.\n",
    "\n",
    "    Args:\n",
    "        model: The Keras model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    MODEL_MODEL_PATH = MODEL_NAME\n",
    "    MODEL_WEIGHT_PATH = MODEL_NAME + weights_keras_string\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, MODEL_MODEL_PATH)\n",
    "    model_weight_path = os.path.join(model_path, MODEL_WEIGHT_PATH)\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Resuming Training where we left off!\")\n",
    "        model.load_weights(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba57699ddc83798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:11:35.740549400Z",
     "start_time": "2024-02-15T16:11:35.668784100Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\"Trains the model and returns prediction model and training history.\n",
    "\n",
    "    This function trains the provided model using the training and validation datasets.\n",
    "    It also returns a prediction model and training history.\n",
    "\n",
    "    Args:\n",
    "        model: The Keras model to be trained.\n",
    "\n",
    "    Returns:\n",
    "        prediction_model: The model used for predictions.\n",
    "        history: The training history.\n",
    "    \"\"\"\n",
    "    #model_load_weights_if_exists(model) # Uncomment this line for transfer learning\n",
    "        \n",
    "    prediction_model = keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE, min_lr=1e-6, verbose=1)\n",
    "    history = model.fit(aug_train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[early_stopping])    \n",
    "    history.history[\"lr\"] = model.optimizer.lr.numpy()\n",
    "    return prediction_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bbc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History\n",
    "\n",
    "def combine_histories(history1, history2):\n",
    "    # Initialize a new history object\n",
    "    new_history = History()\n",
    "    new_history.history = {}\n",
    "    \n",
    "    # Combine epoch data\n",
    "    new_history.epoch = history1.epoch + [e + max(history1.epoch) + 1 for e in history2.epoch]\n",
    "    \n",
    "    # Ensure all keys from both histories are included\n",
    "    all_keys = set(history1.history.keys()).union(set(history2.history.keys()))\n",
    "    \n",
    "    # Combine the histories\n",
    "    for key in all_keys:\n",
    "        # Handle cases where the key might not exist in one of the histories\n",
    "        new_history.history[key] = history1.history.get(key, []) + history2.history.get(key, [])\n",
    "    \n",
    "    return new_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275e398",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5ee96331da27f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T16:11:36.779238200Z",
     "start_time": "2024-02-15T16:11:35.682774600Z"
    }
   },
   "outputs": [],
   "source": [
    "char = len(tokenizer.char_to_num.get_vocabulary())\n",
    "model = models.build_model9v4(IMAGE_WIDTH, IMAGE_HEIGHT, char, LEARNING_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56e45e",
   "metadata": {},
   "source": [
    "#### Training Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842938c05d072b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T16:11:36.763642200Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "prediction_model, history1 = train_model(model)\n",
    "total_duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2df66",
   "metadata": {},
   "source": [
    "#### Training Phase 2 (less learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328aaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(LEARNING_RATE/10)\n",
    "model.compile(optimizer=opt)\n",
    "start_time = time.time()\n",
    "prediction_model, history2 = train_model(model)\n",
    "total_duration = time.time() - start_time + total_duration\n",
    "print(\"Gesamte Trainingsdauer: {time}s\".format(time=round(total_duration)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = combine_histories(history1, history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d09077ade6432b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plot helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1cc631d9374f3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history, name, dir_path, save_fig):\n",
    "    \"\"\"Plots the training history of a model.\n",
    "    \n",
    "    This function takes the training history of a model and plots the training and validation loss\n",
    "    across epochs. It also plots the learning rate on a secondary y-axis using a logarithmic scale.\n",
    "\n",
    "    Args:\n",
    "        history: The training history of the model.\n",
    "        name (str): The name of the model.\n",
    "        dir_path (str): The directory path to save the plot.\n",
    "        save_fig (bool): A flag indicating whether to save the plot as an image.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    metrics = history.history\n",
    "    _, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot für Trainings- und Validierungsverluste\n",
    "    ax1.plot(metrics['loss'], label='Training Loss', color='blue')\n",
    "    ax1.plot(metrics['val_loss'], label='Validation Loss', color='red')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='black')\n",
    "    ax1.tick_params('y', colors='black')\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(0.0, 0.95))  \n",
    "\n",
    "    # Zweite Y-Achse für die Lernrate\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(metrics['lr'], label='Learning Rate', color='green')\n",
    "    ax2.set_ylabel('Learning Rate', color='black')\n",
    "    \n",
    "    ax2.set_yscale('log')  # Verwende logarithmische Skala für die Lernrate\n",
    "    \n",
    "    ax2.tick_params('y', colors='black')\n",
    "    ax2.yaxis.set_major_formatter(StrMethodFormatter('{x:1.0e}'))\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))  \n",
    "    \n",
    "    if save_fig:\n",
    "        plt.title('Name: '+name)\n",
    "        path = os.path.join(dir_path, name + '_history.png')\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e1ef7b215f5b3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Creates a directory if it doesn't exist\n",
    "def create_dir(path_to_dir):\n",
    "    isExist = os.path.exists(path_to_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(path_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6ad48840858cc",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :load_data.max_len]\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(tokenizer.num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140fda6640cf403",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def plot_evaluation(name, dir_path, save_fig):\n",
    "    for batch in val_ds.take(1):\n",
    "        batch_images = batch[\"image\"]\n",
    "        _, ax = plt.subplots(4, 4, figsize=(32, 8))\n",
    "\n",
    "        preds = prediction_model.predict(batch_images)\n",
    "        pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "        for i in range(min(16,BATCH_SIZE)):\n",
    "            img = batch_images[i]\n",
    "            img = tf.image.flip_left_right(img)\n",
    "            img = tf.transpose(img, perm=[1, 0, 2])\n",
    "            img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "            img = img[:, :, 0]\n",
    "\n",
    "            title = f\"Prediction: {pred_texts[i]}\"\n",
    "            ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "            ax[i // 4, i % 4].set_title(title)\n",
    "            ax[i // 4, i % 4].axis(\"off\")   \n",
    "    if save_fig:\n",
    "        path = os.path.join(dir_path, name + '_result.png')\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9bf3e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def create_new_plot_name(model_name, names, format):\n",
    "    \n",
    "    pattern = r\"\\d+\"\n",
    "    max_number = 0\n",
    "    for name in names:\n",
    "        tmp_name = name.replace(model_name,\"\")\n",
    "        number = int(re.findall(pattern,tmp_name)[0])\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "            \n",
    "    new_model_name = model_name + \"V_\" + str(max_number + 1)\n",
    "    \n",
    "    return format.replace(model_name,new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36c4de000ba89f",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace9bfc809bc8f94",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Author: Alexej Kravtschenko (main) and Tim Harmling (wrote)\n",
    "    \n",
    "Creates a new plot name based on existing names.\n",
    "This function generates a new plot name by appending a version number to the given model name.\n",
    "The version number is determined based on existing plot names in the directory.\n",
    "\n",
    "Args:\n",
    "    model_name (str): The base model name.\n",
    "    names (list): A list of existing plot names.\n",
    "    format (str): The format string for the plot name.\n",
    "\n",
    "Returns:\n",
    "    str: The new plot name.\n",
    "\"\"\"\n",
    "     \n",
    "if not os.path.exists(TEST_RESULT_DIR_NAME):\n",
    "            create_dir(TEST_RESULT_DIR_NAME)\n",
    "files_with_model_name = [file for file in os.listdir(TEST_RESULT_DIR_NAME) if MODEL_NAME in file]\n",
    "metrics = history.history\n",
    "\n",
    "NAME = \"{name}_{epoch}E_{height}H_{width}W_{loss}L_{val_loss}VL_{time}s\".format(\n",
    "    name=MODEL_NAME, epoch=history.epoch[-1], height=IMAGE_HEIGHT, width=IMAGE_WIDTH,\n",
    "    loss=round(metrics['loss'][-1],2), val_loss=round(metrics['val_loss'][-1], 2), time=round(total_duration))\n",
    "\n",
    "if not files_with_model_name:\n",
    "    if SAVE_HISTORY:\n",
    "        plot_history(history, NAME, TEST_RESULT_DIR_NAME, True)\n",
    "        plot_evaluation(NAME, TEST_RESULT_DIR_NAME, True)\n",
    "else:\n",
    "    new_name = create_new_plot_name(MODEL_NAME,files_with_model_name, NAME)\n",
    "    plot_history(history, new_name, TEST_RESULT_DIR_NAME, True)\n",
    "    plot_evaluation(new_name, TEST_RESULT_DIR_NAME, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26ab3b04550af2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3721ecbe52011",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Saves the model after training if MODEL_SAVE = True\n",
    "if MODEL_SAVE:\n",
    "    if not os.path.exists(MODEL_DIR_NAME):\n",
    "        create_dir(MODEL_DIR_NAME)\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, \"{model_name}\".format(model_name=MODEL_NAME))\n",
    "    model.save(model_path)\n",
    "    model.save_weights(os.path.join(model_path, f\"{MODEL_NAME}{weights_keras_string}\"), overwrite=True, save_format=None, options=None)\n",
    "    json_string = model.to_json()\n",
    "\n",
    "    with open(os.path.join(model_path, f\"{MODEL_NAME}.json\"),'w') as f:\n",
    "        f.write(json_string)\n",
    "\n",
    "    data_to_save = (load_data.max_len, load_data.characters)\n",
    "    import pickle\n",
    "    with open(os.path.join(model_path, \"handwriting_chars.pkl\"), 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
