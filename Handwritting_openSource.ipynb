{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b545f09-7b22-4a4f-b693-f90de7dfd68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## !pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecaaf7b-b692-4854-a71b-67a21b677dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\anwender\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Anwender\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\~yarrow'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## !pip install -q datasets jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f9ce0323-d185-4e6f-b9ef-04c218f7c87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ag_schueler_name.png</td>\n",
       "      <td>Müllin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ag_schueler_vorname.png</td>\n",
       "      <td>Baninon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ag_schueler_klasse.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ag_schueler_wahl1.png</td>\n",
       "      <td>Robotik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag_schueler_wahl2.png</td>\n",
       "      <td>Band</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name     text\n",
       "0     ag_schueler_name.png   Müllin\n",
       "1  ag_schueler_vorname.png  Baninon\n",
       "2   ag_schueler_klasse.png        4\n",
       "3    ag_schueler_wahl1.png  Robotik\n",
       "4    ag_schueler_wahl2.png     Band"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('tabelle_handwritting.xlsx', header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "31f1feb8-17a9-49d5-bb2b-a10716df98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7aede56a-b7a4-400b-bb85-946a4b9be471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (70% train, 30% validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reset the indices\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a463236f-01e8-4c49-b5ff-5fee06f836f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])  # Convert text to string explicitly\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ceaf03fe-c8f0-4b05-9078-9cfdaf39ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "train_dataset = IAMDataset(root_dir='DataSet_handwritting/',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='DataSet_handwritting/',\n",
    "                           df=val_df,\n",
    "                           processor=processor)\n",
    "\n",
    "test_dataset = IAMDataset(root_dir='DataSet_handwritting/',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c781ad02-a153-4171-8099-cc74fa70eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 22\n",
      "Number of validation examples: 10\n",
      "Number of test examples: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ca4cbb6-7d4c-4a38-9997-7c6395a8b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed4742c3-0b07-4353-bdd0-1b5097295e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAASAHEDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0GwXztLa9ubuZ9QF+0MTNK3ykSYCBeh+WuyuLiG2j3zypGhIXLkAZPauL0+2lsvEWoXfk/aoYLgkQhvmh3gMXUdCTnHrxxWu+oQT6kLhI/tybNqRIQJITjn5GIzn1rrqR5mu39aEI0tWUTaPdgDeDCxGOc8ZpRqtokNuXdi8sSyBUQuQCOpABxUWnwlYpy8Jt4pXykDEEoMYPTgZOTgVl2GtWmk28dheZS7jRQ6ggsey4HU8Y6VCjdNLWwG7HqNnNA88c6skZw+AcqfQjrmpLa8t7pGaGTdtOGXBDKfcHkVgy2VzfznVGtlDZXZZzNjzEGeW9G5yPTp3qWW1uNQuY5IY7jSTGpVplMe9+eFxyCO+TScIdwNLS7ye6gumm2l453RQBjCjGAffmrUDS/ZIzcACbYC4HTPeub0cWloZhdahJNerdyYSWbkknAO0YHI9q1rFmQX8TnJjuHIOMfKwDD+dFSCTdgRNY6kkllZNdSxrPcjChejH2q+a5HQkv7TT7a/nhW9TyhsEbEPCnsp4J9e9dVBPFdW8c8LB43G5W9qVWCi9ATFNB5oI5o59KyGFFH4UY9qACij8f0ooAxl0vT1l3CwtQzZyRCuT+lPXStOIZzYWpbAG7yVz/ACooqhC/2dY4H+hW/X/nkv8AhQul6fnf9htd2MbvJXP8qKKSH0Izptjz/oVt1/55L/hSnTrHcP8AQrfv/wAsl9vaiihAC6Vp3L/YLXdnr5K5/lTRp1jg/wCh2/8A36X/AAoopvYEQXdhZpaSFbSAFV4IjHH6VT0G0tm0OBmt4ixzklBk/MaKKa+B+oLc0PsVp/z6w88/6sUjWNoCP9Fg/wC/YooqAA2drg/6ND/3wKi+yW3lp/o8X/fAoooQD/slt/z7xf8AfAooopDP/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=113x18>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5cb5562-921d-4d8f-ac86-0dd8f379afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robotik\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "16d05ce1-df74-451b-8e15-41c36db24708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3a35b9a9-7c30-4f66-bd2e-83996012c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2e076a5a-1084-4bae-b6bf-82ea816fd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=15,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=False, \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1341d9cc-052d-4371-b952-896508aaf00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "16f98ff8-4310-46e3-a018-73ab423f2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "022190b8-b1c5-4764-826e-6a20a55d29a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 5:31:14, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=0.8686840408378177, metrics={'train_runtime': 20084.241, 'train_samples_per_second': 0.016, 'train_steps_per_second': 0.002, 'total_flos': 2.920154987180851e+17, 'train_loss': 0.8686840408378177, 'epoch': 15.0})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "faebdbdf-2c40-4979-a8ec-fd9dea1ab209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1350111961364746, 'eval_cer': 0.7857142857142857, 'eval_runtime': 234.5377, 'eval_samples_per_second': 0.034, 'eval_steps_per_second': 0.004, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4d97806e-4a35-406a-9cc0-b68740c286b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-22 00:00\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f2203-f9b1-443b-bdb2-4462c3f49778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9fd985-666c-4f7b-bebc-f03c3c3f284f",
   "metadata": {},
   "source": [
    "# Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5028e22-ebfa-4de0-8853-219e3477ee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAeAGYDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3fNOBph65rNvZ783TQ2LRp5UXmsZE3BySQF9uh5qox5mSa9Haqq3qHTResPk8rzSBz2zim6ffG9SQSW728sZAaNyCQCMg8U+V6sC3QKSsJ9QvLjX7GG3Dw2e5hIXXBlIBOADzgetEYOWwXN48Un0rE17Xm0+B47JUmvFBYox4RB1ZvbkAe5rcjO5A3qAaHBpJvqAmaAaRsVm3Av7m4mihla0ijUbZNoYyN+P8I49zSSuBqUdqzre/lbQ4rt41e4KAMiHAL5xgH0zUtndzyTyW11Ckc6AMNjblZT6E03Bq4Fs80UGipAiJ5rKude063jlk83J5QOqEqzDgLuxjOapyeLbBkOYbnBH91f8AGsq31bQ7dUj+zXk0aKQkcxDqg9hnH41UHD7QWZ1SfZbHSrW2vJo4wEUEO4AJHJqPT919/aF1HNsS4fy4pYyDgKMbhn3z+Vcu97okk6NJHqLhEMaKXGAhP3eucfjVy217R7S532trdQLj5oo9ojY9M7c9atzhZ2erCzOrtopYLVIpp2nkUYMjDBb3IrPnUnxLp3JwIZiRnj+EfnzVCTxdZSROix3SEqfmULke45rBN7bnUUv4tU1hZI4zHhxG42nnvUwlG7bdtwszpfE09lHo+op51ulzJF5Z+ZQ5z0z371q2l9aXGIbe5imZEBIjYNgdM8Vyyazo/L3FnNdzdTLcRxs2MdPpVyLxVp0Ef7qzlRT2RVH8jQ5R5eULM3re0jtWnMbP++fzGDNkA4xx+VY1z4s0yKWW3jkd7gMY0XbgNJ025+vrxTW8Y2QIX7PcZIz/AA/41Wk8Q6S5lV9PYmUbZDsXLA+pojKN7z1CzLhnu7MwaVaQW800EAmkadyik5/h9yc89BVjSb0avfPfQqVhji8kg9fMzlh744Gaw4tX0owRw3VrLdhOI2nVGKr2Ge9Sx65YRXBmtYri3DAB0jVNrAdOOxxxkVbnCz7hZnXmiq1heJqFolxGrIrdA3WisQP/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGYAAAAeCAIAAABRzPyZAAAVxUlEQVR4ATWZ+ZNcV3XH397L9DK7RttoRotlyWDZyGAbTAIWOOUlEBIqPziQgiw/pPInQaWSSqWgSCAInARbxvIiYysGC0QkJGuXZuvRaJaeXt7+8vneN3pS9bx+796zfM/3nHvubbsXRkVR2OayLKuwCtuxirxwbZsvaV44tu26bsE7m5f6W1i2ZWsUk5jLpXd6w2Ob92Yqg/Sf6Wmea7i5sixHGrd5njHS0b0k8M1xHG6QwCuHaRIu+dxmklA+MQbsmKFXXEhjmLFB0zGifMDbUuDDATKv0HtU5LzFE7zARCOPT/TkpRkYbmTKKs0y71zHybJM340vEsRrx0CBHN6BkOs6qOCr/plLA8wTy0Yos5jjGjMkByE8EUAG3hzQ07zEBK0SUbph2a7j6rvgx2RFxcw1Ngg7DcUao1NiH+rXA14wSTfmQnB5wyc+m7jaOVGQe7Kk9IW3RhlPhJtRIakyrGBejjzX9Ur7BQVajIodRRDJAOqglkdcfGequcMk13LcFLJJjIMGLknnH6RDmPTL4tI3M01fGaZPw0NelVCWMCDcjMc8kQp1rucZY3kuAJjHpzFDYjAHuhU84AvvAVZW6CpFYQUqoG1pA4+RL5QNahhphDNbPJJvBnbkG/MkRabigoHQsE9sYCCfaCmlyQYstnNsMBjmTl4oJRQ0E39jEv6kPHBttzAUkRukhjAz8Ueq5PKBJUjMDFbSIU3ySQpxSeAY28rhxBdCMsL4gWuZoSxGKUmJIohhBdSWecoXE6wSND2U0Ua+8kSeGy18ikfyLTXu4Q1iGOxmCBWGSJa5xjR9lk/kl/RaSDOFQ9pRzUi5xzgVFr2XIcjRCyDIGKRnKNvx2WCn7xQyw0I+cYMnskQoSKBxx+SgIXk5V9YIRSqCeSDLib5Rb7TIaXNhh4wjfjhuWXAFLplbzEWFeCijkaDxgsIYQEKL5aUf5r2A45GpPkhywKdEhMmYwRjf9ZgmQYg0UiWBoRipMfIC4TyRThkkkxihh4qeHABMjdM33ha5gl/YWUqUMF7YyUC+ZCkvkMW9kcMtk5UOutPFTGk1w413ZrACKHWymIsXTE8kH82lUqGCBVhoHEYFAyVNjJYvXPBUqhGmf6YqpWnKSyoHjw25jAKqmCyQz0zDh8wklJBVDVCySJ5hJc/QgzbltITzB3dwWVYaSGWHRmgMxgh45gKj7biCEs2aJUmiDu/0pQyC66aqFrxi5M5zRpSrnkwoQTHjy1k8KWMoS0uLKMPSrtWHGKDBfJGJxj7dlGKMYSWgpLDQhKVcpWSDkTHW4GLY5qQZBpKSpDN6Lc+jyFiOJyE8BOfSVN2a9Cz9RjIxEKyMI2KeBxqYi5/GOOUKpdzoFvdACRjIHpSxQFBZRDQAVDegL4qYZrKQiT2kDzDITS5JJae07Oh/ecEbLNJM1QJYt+Ok7Cv5YJw0/HJQzUh8ojpit2V5D6ejhaKGAHoaZawQhP+Mgo9UYCdjnuquFEmFXDKXgUOgeL6QljFM1nzRBDPNEsZTgqe2hkQM7CDLYmAztjBWSSGHJFauIdjgzDPVfumXL5aHWKqDeKxGicEqE8COQbpQQA21UtsJhKoWDtYEp7ASLFJVM7DhgvkrH3iC6+ar5KHVGFEO4AG1hqJsqd4xWn7pxvMJI55SzhKKDzNNMAxuTurqCasn3so3+IVMhiqdkLHzqTKjEMhrfcjC0jyGQEJZAyiEybOKxC5Y4uSv6oOpFXADmbKNtDCPqBlMAnRiyFgYjhBmiRJGsUZLkZmPNhE+LZwMqlVQ6PtaGXlMcyqDheyOuUYgPKT0wFCVEiRBEIiobDFX6RtE4Ka0h3RCHCxfX+vf+mR7eyMhtywr0GSeG2PwJ8/ILZ6r3qKfyQxANVXloaideAAQlY5PIwAZaOMefVYSZmuLcTjIAEFQY7/yTSFDCDNkGI9l3k4ul2ZjKksHaiF8CRcsQzRiTf+Vkqu5yRNFkrW/yCLH9zpL2dVrm4HvPPbY+PikTwEQq8T3EmtlvghjAo5Ag6xZcZydelyqN76Yt9pUQLccczY3+r88c/PmteDUC3ufftbLyFqhpbf4SmvZ6yWrW8PpmQY7BVkKXjJa9/wBO274BB0M8DyPiWY6ycGN71n+5YsPLvx+c31tY9dI89kvzR54xKfUEAMgMtYSiJyCZkDRcl86Ijm6M6QyQ4HPlAgliAyADerAscKUESzAK8vxNzesN9+6e/3u4Ox79147vbD5gNojBBQ7cyn1rJyi6IsGxnAiIKfkHkrl50M0ZeNDf+ELVL93Z3DtehQ5bnOqmqqcADNszRPoladhLzv9gxs//eHCMESocsIQQulnvCmppPKPivIyCkRGIAg8762zN8+dXzlxcvqFrx3etqL/+MHvB13ySws3LqstLYrtbnbrZu/ypfXVTkRuoALh5r3kQz1MlhsmSligNJJvJmiiD+aaNSiXB/aFjxf7ofP1b8x9868f66bZxxe2Cb+hGBKQpjjHveTC+dX//vnyrWsxgCIJkcYrcMURecEfx1M2KWDSUviOwwb3wsVuN/WqTbfdIgEIrCIABrgTeP6Z1299fHF1eTXa2AqNDHDQzsS8F2o8dLRGaKtkaozcoP5AU7J5aWHj/K/vv/DS3GNH/EfmWl96cf/S/c69hWFBepIl2u96a2vJ6dOdN95cPverzmv/tbR4L0EOKyEWIgjeGInKQ+OYBTlclk/cBlS8yvNEJpk+iyAkWbS0mhGiybY90a52Hmt1Fvtx1jY7RIoaF+Wj+PijlV/9b29ksr3cWZ2e3N2eoFqbbhx9qtECV5f4SwUSifpbve2t/MMLg7Ve8ejxqYpba4xAW8yy8yQN07xRdxbvDC5eGc4e3xdu0Y6qCBmwxSmlopwwVxmdHZuJBv89kE/C4qenf713fmruQJCEKdvduT21p7/6xFZEArkgxu6mP4iv3LpfH/deef4gwXz7neX3P3zw8uhMBWNECJOkhl5ShyISz2iXrWkqU8ouBkNES+q+FfRDe2zMS5M0C9PZ/fWRZhDFgGWYxIdTJElx7W721LMzr/7VHitIOhvgIrEEiP+IAtfSNdFLt4XnuX+41vnZmcXtMP7Gn88e3RtYcYQ2WeXa61vx0t2wt53++D8vHTg88eljI61WEfiqTHQJSCsjKhoUKnlJErOGoArZhhyoSUnwMM6/+OXPvPzSsXjAKmwQiOyxEacYpiyKaUHqs64FTl574sToeNNvNb3PPTsNVxfuhZ6rTBca4rMml3FHh5jFtfPwoXsyhotRtj29pzI9WbXcwPbd0dGgs7C5fp+9IZjxXhAMh2m94Z787Gi7UbTHaleuP9AZCLJNiSz/4CSDuShQBA9eP3rs4Ff+ZO7PvrZnesJbXg2pDiwv1BC1H767cHv4z9+7fO9B/MJLM82GfX+r2xvEphxAjtIuqKT8xSsqdxLGpRcgKMMFfl5t2UePtUerqo+mNDFTVFxfG+SQnfKfp4VbpFE8PuZZ3Bd5zbPG2u6DbqKmUVmhIBt4kKscQbcg018TBFQxSLvxndEFsZjd7VR8J0sTCE9jGGVWGGm9wQj8J+QrS8ODc41mk2WoGBuvDVMvofNRh6y+AIfUIrDIOIVLMVPuEzO7Xi1mZ/xGkBcxzYxVb6gXU89aWBNtvzLhBNOtv/nuiV1tLMyzIV0C6nFeuCmYJloAHIXJR+fXfnHmFj7R88NfrSpaDc3CKgvZDEgydPE9J43InEpmZbLN8TY2opFRv9VS5wD0dKitlj0cxmZbADIGbMNr5aC8Rix3O1aYvyDPnkaXQdJOjh7d5fnmRIDhhe0HbrWmfQKzCfCgn/UG0fzhFuBmCdmdBJSIzFLlyIoblzoXzq/0tpjO4ojdShAMKLWbxtJZXNq4c3et0azQT+htYVOHnvncxHe+dfjI/koaZ9WKm6ex75eF2KDFOMnR+E4nXFiKLl+Orl3radVXRcE0U0BN36WaZQEk7StFMhnE2eQMELoc5OVkXsV75JFRF/PUURbNdi2oeN0ttYMIL2MjnBR+t2zcvIcmiOFl11uOI2igXmHPRsyEq/zMUqcSOJWK4T0jsMwuDj4y3h5T2SMi3fXQqwaKV+6ncfy9fzmTx/PPf7U49eK0WdC0mhIZYw19ALZkSWLHsV2pEmct7WhHLtsRO4PF7D3CPM23tvo0QCg0TJc1SGABoGbNzrcqLb8XmUpsw2+lCHSBVmDKeI1mOJs/z93aiJZWhidOTjnotXOvCGfaFURRImONYZNR9LejMMqo44YyLG5aeRVn2G5IZSDT4q76gtwspb3EExNBwatW3nN9xcAqupscbNgjDRYvzKA0ZhWfXQGDrDhNVzu9tbX0yafHtVUsANf/9j+8fOHSVt+Jw8yuGOhRjYlcWjbFenff/tGqfz8cRrbVkGc7mcuaBrSZJ4K4QaWCjdgE0+SZ7FMtk5Q0nxr1v/3tOZr2PGUbp5UHQgl40+Go4lGZtE/079wdUtfG2p7ss72VO+HvLnaiQfj4idFDx6cSlStrbSlt0DU76gQAAM6aYiC1YE/WKxnIBRVsXKAjNbscExjgy9j68RTxvix2Oyu9RlOHT6YBFFlEV0yCDYWzshJvD7M9u3xO/FKZ6Dw63zoy28Z+QFTTblSYoMsCUojjJuojwXNN7AQCMuEYWaOCyRGDvbkxaI9WWs1KDjNseCABlDTcE8lhPjCmLP707ToWA1kAwzb0yVQ1Viws8Cq7cb17/FNTrRbavU+ub/zwXy83JupBLT/3/Vt/+cqR575yaGl5cG+5/8en9jBCxBLwOC4+I1ARYqYBTzWS/8KRsZwYFBHcDoIgi63NjSSKBuOjQbWeXbszPHJsvB7YoTbQwprUsHyHasl6t7EZ7tvX3DVdkb3aBSALZdiMZHnisHSDGgipUohPvu9vb+dx4oyOVhmOgaUNapk0RkfG/dAdnxhTtMkbtroUEC0TCNCxBMsKIlFB9THZI2awYPC5uT5cXoiuXn9wYF/r08cm3v1w5c5i74WX5yBmmiXnP1xptFt//3dHvcB5/Z3Fn/3izvTe3Z9c7U7tG3n0WGOnlAGHmmT8lL2gQ3pSQVAoj3CPeGMKxrkFRdw6/6vO7YV0e5hs3x+OTdZG66zK7uHDDYqoQJFbTn/TijKWKqefRAvX4y+/NGWKhwqNwUVpKJB4qvVfz6WLpU+lgAZNBYJzk2ZdI4QY3utXAXMKZOdRlC12ova453h5OiyoeksL67V6MTc/YQ6EtO/Rimsu9Ki8aO3zbt3eeO/s7Xt3k/1zMx+c75z+ycXG2O4//eY8nVCc5Cur4dUrK6+++tT4mJvE2V+8eKBW8X/075drwcjXvzVbZfFn0RVbldDqN8xxC99hq6dW2XgFkGKCYHMD13vn7dtvnV1+/sUjzx/fyzr4znuLr5258vxzx1pVmoXcT1x+sHv33YX/u9of3z22tXx/sJ3XmvXmCERQwhhaacXgQuzOp9CTc+QPkTN6tUKzieI0KS+qPGHs5sbw7BvXiswDzWHI7i+MoqSzuDU20cTSlXvd51/Yt/ewTsK0iJrW7KEWgxxpiiDP+e3F9eld7STtrW4Pen7NDreHawm7QMg4MVkfHatcubR6YN8MoR9sd/srg7vLg5OfGds9UytS+KP4ynKta1gtmJQytI69MDQLpYqt2eOojV673/+n739y/Kl9X3tlX96PL3/SO/ebtanZZufO+nSz9crLe2pt++zrN99/r/vUc3vn55r/c+bO8mp3vFkbrTRefGX2yCNVejfco4pSEHXjg5E0YwVq2QMKL8uqusHZX944c+bB3/7jiQP7VTgpCHTlH/927d23l1i5qnWnPeqz3ZmYbLTaI1XfOTDXmjtQs7MEMpY9EjgxEcFQQyqgMKnrWB9+tHr7bpclYXKm8vmn9/zhyua5t5b+6LkDzz4zkVvxpatbZ964UwGvkEXbak3Wn3hi5tfnVj/z6ZlTp0ajRFslQVamB+2DQqwtulo444UOLuQKy3SaDWKrn4Xhdvr6azeWO4MkrX3+C7tOPNm435k6/dObP3/t3he/sGdpMR4Oi+Egf/uN25PjzVe/dZQ24N1ziz8+ffvkk+O1Wv7EyTFJ1NJIneESjznIrHjBrevrly8OajUfSF97/aJjjf7uowftYCJJIvZn3W62vjj47neOs/h4rlVlFGsMxd3KfR13sUXiyFP8ok4a5KWGjIYCuAMDRInM/vxnpz93cgqlrpdy/vrMyQl+fHjzzRsTTX9md+3okeb83OMb3agXJhPt+viI7/hpo+K8/vPO/v2VQ/N1iJ/SbNAd2xyo6gBWtOJBdzhQpVEu8dBDZ0zBsLwbt7dvXO9Dy13T9UPzjYkxGpHCC7yNfvHBB52NB1FrfKQ/GAa+f2C2eexIo0azltOzWb+/vHHh47XHjk0++WQLhERlExboQ10AtM2t6Ef/doVzuTC0uv3wxs27DxY3Dx0/fPDQ7s3N4dhYfWImmNlb+eIzu3w3zhKRhgMQDoKQQJEy5/XgQIgx2yxCAo/koCxq06Pehe9GrVhBrDy8rVh5FFvBL8+uXPlD91OHx06dmkAYVZOChVkM10+RNf+D9x98cqm3d69XqwQHDzVm5yoskiqxkIwmkihR3ZFr2jQdKQKWTrTZ8bC6sTixbPusmPTt2vF4QYD7FH9MqwVqqZMsUd/KuSAFQFVCbUGmsxUpUZennNFZPnTAJ45twoQDEjuouHRr1Ov17eji1XU78hHYmqjOzzdaI07AgkKfohVCmQFUJJ2yRHQSXiYK+qHDsLc8f+apTpLV5BIZnVXyD3qIco7DDhY/0ex9+Jvu1ETw6MEWcx1ayiLhBwDbq4Ipc93A76xGHArUq/6uXYHrsMMnFJjvek4RE4D+9n2YmwglKB3p4Me3E8pJHGE4W0vMEgXNLxmchfl+hcKYxhw5sSgmvlOly2eXQDmHAnHYdR32jVZQ9aMYDtaojGlu+xU/TfpQW/YTBp33AqfWgWJYeHVIgZaULiSl9+KnVW1YIaiSGreTJA0qfhyRjzoF0VaQY8iU449htabzK2a5HkazA/LTdBiXXQjdNjap5U5tjEorIBP2NyqVesbxfAbvaPozfmCpYp7y2U3SIeUyS+IKWcP2I7XCOK5VK/VGy2mMqp6lhfejH/6kNlKLcYsgZhQLharfozrkQc2i1QTNJM+CoI7uOOs3RxrUBTBMHDqA7Xp1HO7TlLmZG8GKfMjPHbbvcabCqsN+K0s4amFlg7B5mLpu1apWKr2tHrBbtjr7LE7brXqYRkAehonHfqxSjdIBGzY2m/xaRhD79GZqW5xao0oqZmHk+biXx2lRxTcIYFtREnW7/QatNknm2NVaHc/zNB0MosZIi94vKax64A/isBrUWRK1WACJx6FSEQRFFA8sr8ayUIVucV6pBPTiMbikHM80Hj/5+FijTTLBgP8H+Au9L0713WoAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=102x30>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "path = \"DataSet_handwritting\\\\test_ds\\\\ag_schueler_datum.png\"\n",
    "image = Image.open(path).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d33e22fe-3d59-4cec-8626-367f8f9b7cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# calling the processor is equivalent to calling the feature extractor\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "print(pixel_values.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e973782c-7e9f-4cfa-a671-6b9cf81e6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model_2 = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1d8c87cb-a37c-4526-a132-91f14357f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.11.22\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model_2.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88371469-8ccf-4ee7-aef4-d3a318c546f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
