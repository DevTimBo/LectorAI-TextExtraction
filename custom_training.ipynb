{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9onjGZWZbA-"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T01:38:41.974301Z",
     "iopub.status.busy": "2023-10-04T01:38:41.974029Z",
     "iopub.status.idle": "2023-10-04T01:38:44.942436Z",
     "shell.execute_reply": "2023-10-04T01:38:44.941645Z"
    },
    "id": "jElLULrDhQZR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import load_data\n",
    "import load_transfer_data\n",
    "import models\n",
    "import modelz\n",
    "import hyperparameter_tuning.learning_rate_scheduler as lrs\n",
    "\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_img_paths, y_train_labels = load_data.get_train_data()\n",
    "#x_test_img_paths, y_test_labels = load_data.get_test_data()\n",
    "x_val_img_paths, y_val_labels = load_data.get_validation_data()\n",
    "#x_train_img_paths, y_train_labels = load_transfer_data.get_train_data()\n",
    "#x_val_img_paths, y_val_labels = load_transfer_data.get_validation_data()\n",
    "print(f\"Training path: {x_train_img_paths[0:2]}\", y_train_labels[0:2])\n",
    "print(f\"Validation path: {x_val_img_paths[0:2]}\", y_val_labels[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "\n",
    "train_ds = tokenizer.prepare_dataset(x_train_img_paths, y_train_labels, (IMAGE_WIDTH, IMAGE_HEIGHT), BATCH_SIZE)\n",
    "val_ds = tokenizer.prepare_dataset(x_val_img_paths, y_val_labels, (IMAGE_WIDTH, IMAGE_HEIGHT), BATCH_SIZE)\n",
    "#test_ds = tokenizer.prepare_dataset(x_test_img_paths, y_test_labels, (IMAGE_WIDTH, IMAGE_HEIGHT), BATCH_SIZE)\n",
    "aug_train_ds = tokenizer.prepare_augmented_dataset(x_train_img_paths, y_train_labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = len(tokenizer.char_to_num.get_vocabulary())\n",
    "model = modelz.build_supreme_leader(IMAGE_WIDTH, IMAGE_HEIGHT, char, LEARNING_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = lrs.lr_scheduler(\n",
    "    initial_learning_rate=LEARNING_RATE,\n",
    "    decay_steps=10000,\n",
    "    alpha=0.0001,\n",
    "    warmup_target=None,\n",
    "    warmup_steps=0,\n",
    "    name=\"cosine_decay\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoint.h5\", save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\"Trains the model and returns prediction model and training history.\n",
    "\n",
    "    This function trains the provided model using the training and validation datasets.\n",
    "    It also returns a prediction model and training history.\n",
    "\n",
    "    Args:\n",
    "        model: The Keras model to be trained.\n",
    "\n",
    "    Returns:\n",
    "        prediction_model: The model used for predictions.\n",
    "        history: The training history.\n",
    "    \"\"\"\n",
    "    prediction_model = tf.keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "    history = model.fit(aug_train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
    "    history.history[\"lr\"] = model.optimizer.lr.numpy()\n",
    "    return prediction_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler())\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "prediction_model, history = train_model(model)\n",
    "total_duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y2VSELvwAvW"
   },
   "source": [
    "### Training Loop with GradientTape (needs fixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(\"train_loss\", dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"train_accuracy\")\n",
    "val_loss = tf.keras.metrics.Mean(\"test_loss\", dtype=tf.float32)\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"test_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T01:38:48.356465Z",
     "iopub.status.busy": "2023-10-04T01:38:48.356233Z",
     "iopub.status.idle": "2023-10-04T01:38:48.370181Z",
     "shell.execute_reply": "2023-10-04T01:38:48.369528Z"
    },
    "id": "tMAT4DcMPwI-"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train, training=True)\n",
    "        loss = loss_object(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_train, predictions)\n",
    "\n",
    "\n",
    "def validation_step(model, x_val, y_val):\n",
    "    predictions = model(x_val)\n",
    "    loss = loss_object(y_val, predictions)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(y_val, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f\"logs/custom_train_tb/{current_time}/train\"\n",
    "test_log_dir = f\"logs/custom_train_tb/{current_time}/validation\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T01:38:48.431023Z",
     "iopub.status.busy": "2023-10-04T01:38:48.430784Z",
     "iopub.status.idle": "2023-10-04T01:39:14.324052Z",
     "shell.execute_reply": "2023-10-04T01:39:14.323356Z"
    },
    "id": "AIgulGRUhpto"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    for x_train, y_train in train_ds:\n",
    "        train_step(model, optimizer, x_train, y_train)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"accuracy\", train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for x_val, y_val in val_ds:\n",
    "        validation_step(model, x_val, y_val)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", val_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"accuracy\", val_accuracy.result(), step=epoch)\n",
    "\n",
    "    template = \"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}\"\n",
    "    print(\n",
    "        template.format(\n",
    "            epoch + 1,\n",
    "            train_loss.result(),\n",
    "            train_accuracy.result() * 100,\n",
    "            val_loss.result(),\n",
    "            val_accuracy.result() * 100,\n",
    "        )\n",
    "    )\n",
    "\n",
    "for callback in callbacks:\n",
    "    callback.on_epoch_end(\n",
    "        epoch + 1,\n",
    "        {\n",
    "            \"loss\": train_loss.result(),\n",
    "            \"accuracy\": train_accuracy.result(),\n",
    "            \"val_loss\": val_loss.result(),\n",
    "            \"val_accuracy\": val_accuracy.result(),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "\n",
    "def plot_history(history, name, dir_path, save_fig):\n",
    "    \"\"\"Plots the training history of a model.\n",
    "\n",
    "    This function takes the training history of a model and plots the training and validation loss\n",
    "    across epochs. It also plots the learning rate on a secondary y-axis using a logarithmic scale.\n",
    "\n",
    "    Args:\n",
    "        history: The training history of the model.\n",
    "        name (str): The name of the model.\n",
    "        dir_path (str): The directory path to save the plot.\n",
    "        save_fig (bool): A flag indicating whether to save the plot as an image.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    metrics = history.history\n",
    "    _, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot für Trainings- und Validierungsverluste\n",
    "    ax1.plot(metrics[\"loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "    ax1.plot(metrics[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"black\")\n",
    "    ax1.tick_params(\"y\", colors=\"black\")\n",
    "    ax1.legend(loc=\"upper left\", bbox_to_anchor=(0.0, 0.95))\n",
    "\n",
    "    # Zweite Y-Achse für die Lernrate\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(metrics[\"lr\"], label=\"Learning Rate\", color=\"green\")\n",
    "    ax2.set_ylabel(\"Learning Rate\", color=\"black\")\n",
    "\n",
    "    ax2.set_yscale(\"log\")  # Verwende logarithmische Skala für die Lernrate\n",
    "\n",
    "    ax2.tick_params(\"y\", colors=\"black\")\n",
    "    ax2.yaxis.set_major_formatter(StrMethodFormatter(\"{x:1.0e}\"))\n",
    "    ax2.legend(loc=\"upper right\", bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "    if save_fig:\n",
    "        plt.title(f\"Name: {name}\")\n",
    "        path = os.path.join(dir_path, f\"{name}_history.png\")\n",
    "        plt.savefig(path)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_keras_string = \"_weights.keras\"\n",
    "\n",
    "\n",
    "def create_dir(path_to_dir):\n",
    "    isExist = os.path.exists(path_to_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(path_to_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][\n",
    "        0\n",
    "    ][:, : load_data.max_len]\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(tokenizer.num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def plot_evaluation(name, dir_path, save_fig):\n",
    "    for batch in val_ds.take(1):\n",
    "        batch_images = batch[\"image\"]\n",
    "        _, ax = plt.subplots(4, 4, figsize=(32, 8))\n",
    "\n",
    "        preds = prediction_model.predict(batch_images)\n",
    "        pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "        for i in range(min(16, BATCH_SIZE)):\n",
    "            img = batch_images[i]\n",
    "            img = tf.image.flip_left_right(img)\n",
    "            img = tf.transpose(img, perm=[1, 0, 2])\n",
    "            img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "            img = img[:, :, 0]\n",
    "\n",
    "            title = f\"Prediction: {pred_texts[i]}\"\n",
    "            ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "            ax[i // 4, i % 4].set_title(title)\n",
    "            ax[i // 4, i % 4].axis(\"off\")\n",
    "    if save_fig:\n",
    "        path = os.path.join(dir_path, f\"{name}_result.png\")\n",
    "        plt.savefig(path)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_plot_name(model_name, names, format):\n",
    "\n",
    "    pattern = r\"\\d+\"\n",
    "    max_number = 0\n",
    "    for name in names:\n",
    "        tmp_name = name.replace(model_name, \"\")\n",
    "        number = int(re.findall(pattern, tmp_name)[0])\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "\n",
    "    new_model_name = f\"{model_name}V_{str(max_number + 1)}\"\n",
    "\n",
    "    return format.replace(model_name, new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Author: Alexej Kravtschenko (main) and Tim Harmling (wrote)\n",
    "    \n",
    "Creates a new plot name based on existing names.\n",
    "This function generates a new plot name by appending a version number to the given model name.\n",
    "The version number is determined based on existing plot names in the directory.\n",
    "\n",
    "Args:\n",
    "    model_name (str): The base model name.\n",
    "    names (list): A list of existing plot names.\n",
    "    format (str): The format string for the plot name.\n",
    "\n",
    "Returns:\n",
    "    str: The new plot name.\n",
    "\"\"\"\n",
    "TEST_RESULT_DIR_NAME  = 'test_results'\n",
    "\n",
    "if not os.path.exists(TEST_RESULT_DIR_NAME):\n",
    "        create_dir(TEST_RESULT_DIR_NAME)\n",
    "        \n",
    "files_with_model_name = [file for file in os.listdir(TEST_RESULT_DIR_NAME) if MODEL_NAME in file]\n",
    "metrics = history.history\n",
    "\n",
    "NAME = \"{name}_{epoch}E_{height}H_{width}W_{loss}L_{val_loss}VL_{time}s\".format(\n",
    "    name=MODEL_NAME, epoch=history.epoch[-1], height=IMAGE_HEIGHT, width=IMAGE_WIDTH,\n",
    "    loss=round(metrics['loss'][-1],2), val_loss=round(metrics['val_loss'][-1], 2), time=round(total_duration))\n",
    "\n",
    "if files_with_model_name:\n",
    "            new_name = create_new_plot_name(MODEL_NAME,files_with_model_name, NAME)\n",
    "            plot_history(history, new_name, TEST_RESULT_DIR_NAME, True)\n",
    "            plot_evaluation(new_name, TEST_RESULT_DIR_NAME, True)\n",
    "elif SAVE_HISTORY:\n",
    "            plot_history(history, NAME, TEST_RESULT_DIR_NAME, True)\n",
    "            plot_evaluation(NAME, TEST_RESULT_DIR_NAME, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR_NAME):\n",
    "    create_dir(MODEL_DIR_NAME)\n",
    "model_path = os.path.join(MODEL_DIR_NAME, \"{model_name}\".format(model_name=MODEL_NAME))\n",
    "model.save(model_path)\n",
    "model.save_weights(\n",
    "    os.path.join(model_path, f\"{MODEL_NAME}{weights_keras_string}\"),\n",
    "    overwrite=True,\n",
    "    save_format=None,\n",
    "    options=None,\n",
    ")\n",
    "json_string = model.to_json()\n",
    "\n",
    "with open(os.path.join(model_path, f\"{MODEL_NAME}.json\"), \"w\") as f:\n",
    "    f.write(json_string)\n",
    "\n",
    "data_to_save = (load_data.max_len, load_data.characters)\n",
    "\n",
    "with open(os.path.join(model_path, \"handwriting_chars.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/custom_train_tb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_training_walkthrough.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
