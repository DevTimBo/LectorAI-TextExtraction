{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9onjGZWZbA-"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T01:38:41.974301Z",
     "iopub.status.busy": "2023-10-04T01:38:41.974029Z",
     "iopub.status.idle": "2023-10-04T01:38:44.942436Z",
     "shell.execute_reply": "2023-10-04T01:38:44.941645Z"
    },
    "id": "jElLULrDhQZR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length:  29\n",
      "Vocab size:  67\n",
      "TensorFlow version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import load_data\n",
    "import load_transfer_data\n",
    "import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples: 10209\n",
      "Total validation samples: 567\n",
      "Total test samples: 568\n",
      "Maximum length:  93\n",
      "Vocab size:  79\n"
     ]
    }
   ],
   "source": [
    "load_data.print_samples(IAM_DATASET_PATH)\n",
    "x_train_img_paths, y_train_labels = load_data.get_train_data()\n",
    "x_test_img_paths, y_test_labels = load_data.get_test_data()\n",
    "x_val_img_paths, y_val_labels = load_data.get_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer\n",
    "\n",
    "train_ds = tokenizer.prepare_dataset(\n",
    "    x_train_img_paths, y_train_labels, (IMAGE_WIDTH, IMAGE_HEIGHT), BATCH_SIZE\n",
    ")\n",
    "val_ds = tokenizer.prepare_dataset(\n",
    "    x_val_img_paths, y_val_labels, (IMAGE_WIDTH, IMAGE_HEIGHT), BATCH_SIZE\n",
    ")\n",
    "test_ds = tokenizer.prepare_dataset(\n",
    "    x_test_img_paths, y_test_labels, (IMAGE_WIDTH, IMAGE_HEIGHT), BATCH_SIZE\n",
    ")\n",
    "aug_train_ds = tokenizer.prepare_augmented_dataset(\n",
    "    x_train_img_paths, y_train_labels, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = len(tokenizer.char_to_num.get_vocabulary())\n",
    "model = models.build_model9v4(IMAGE_WIDTH, IMAGE_HEIGHT, char, LEARNING_RATE)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=LEARNING_RATE / 10,\n",
    "    decay_steps=500,\n",
    "    alpha=LEARNING_RATE / 100,\n",
    "    warmup_target=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    name=\"cosine_decay\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"model_checkpoint.h5\", save_best_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=EPOCHS, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y2VSELvwAvW"
   },
   "source": [
    "### Training Loop with GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(\"train_loss\", dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"train_accuracy\")\n",
    "val_loss = tf.keras.metrics.Mean(\"test_loss\", dtype=tf.float32)\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\"test_accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T01:38:48.356465Z",
     "iopub.status.busy": "2023-10-04T01:38:48.356233Z",
     "iopub.status.idle": "2023-10-04T01:38:48.370181Z",
     "shell.execute_reply": "2023-10-04T01:38:48.369528Z"
    },
    "id": "tMAT4DcMPwI-"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train, training=True)\n",
    "        loss = loss_object(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y_train, predictions)\n",
    "\n",
    "\n",
    "def validation_step(model, x_val, y_val):\n",
    "    predictions = model(x_val)\n",
    "    loss = loss_object(y_val, predictions)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(y_val, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f\"logs/custom_train_tb/{current_time}/train\"\n",
    "test_log_dir = f\"logs/custom_train_tb/{current_time}/validation\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-04T01:38:48.431023Z",
     "iopub.status.busy": "2023-10-04T01:38:48.430784Z",
     "iopub.status.idle": "2023-10-04T01:39:14.324052Z",
     "shell.execute_reply": "2023-10-04T01:39:14.323356Z"
    },
    "id": "AIgulGRUhpto"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    for x_train, y_train in train_ds:\n",
    "        train_step(model, optimizer, x_train, y_train)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"accuracy\", train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for x_val, y_val in val_ds:\n",
    "        validation_step(model, x_val, y_val)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"loss\", val_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"accuracy\", val_accuracy.result(), step=epoch)\n",
    "\n",
    "    template = \"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}\"\n",
    "    print(\n",
    "        template.format(\n",
    "            epoch + 1,\n",
    "            train_loss.result(),\n",
    "            train_accuracy.result() * 100,\n",
    "            val_loss.result(),\n",
    "            val_accuracy.result() * 100,\n",
    "        )\n",
    "    )\n",
    "\n",
    "for callback in callbacks:\n",
    "    callback.on_epoch_end(\n",
    "        epoch + 1,\n",
    "        {\n",
    "            \"loss\": train_loss.result(),\n",
    "            \"accuracy\": train_accuracy.result(),\n",
    "            \"val_loss\": val_loss.result(),\n",
    "            \"val_accuracy\": val_accuracy.result(),\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, : load_data.max_len\n",
    "    ]\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(tokenizer.num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def plot_evaluation(name, dir_path, save_fig):\n",
    "    for batch in val_ds.take(1):\n",
    "        batch_images = batch[\"image\"]\n",
    "        _, ax = plt.subplots(4, 4, figsize=(32, 8))\n",
    "\n",
    "        preds = prediction_model.predict(batch_images)\n",
    "        pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "        for i in range(min(16, BATCH_SIZE)):\n",
    "            img = batch_images[i]\n",
    "            img = tf.image.flip_left_right(img)\n",
    "            img = tf.transpose(img, perm=[1, 0, 2])\n",
    "            img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "            img = img[:, :, 0]\n",
    "\n",
    "            title = f\"Prediction: {pred_texts[i]}\"\n",
    "            ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "            ax[i // 4, i % 4].set_title(title)\n",
    "            ax[i // 4, i % 4].axis(\"off\")\n",
    "    if save_fig:\n",
    "        path = os.path.join(dir_path, name + \"_result.png\")\n",
    "        plt.savefig(path)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/custom_train_tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_keras_string = \"_weights.keras\"\n",
    "\n",
    "\n",
    "def create_dir(path_to_dir):\n",
    "    isExist = os.path.exists(path_to_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(path_to_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR_NAME):\n",
    "    create_dir(MODEL_DIR_NAME)\n",
    "model_path = os.path.join(MODEL_DIR_NAME, \"{model_name}\".format(model_name=MODEL_NAME))\n",
    "model.save(model_path)\n",
    "model.save_weights(\n",
    "    os.path.join(model_path, f\"{MODEL_NAME}{weights_keras_string}\"),\n",
    "    overwrite=True,\n",
    "    save_format=None,\n",
    "    options=None,\n",
    ")\n",
    "json_string = model.to_json()\n",
    "\n",
    "with open(os.path.join(model_path, f\"{MODEL_NAME}.json\"), \"w\") as f:\n",
    "    f.write(json_string)\n",
    "\n",
    "data_to_save = (load_data.max_len, load_data.characters)\n",
    "\n",
    "with open(os.path.join(model_path, \"handwriting_chars.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(data_to_save, file)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_training_walkthrough.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
