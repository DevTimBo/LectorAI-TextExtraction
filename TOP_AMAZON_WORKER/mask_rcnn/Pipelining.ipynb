{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fd2c0b41-36a3-44ee-beb4-92c2440638f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import json \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "728f7269-00d2-49a1-ba83-02f2d6d16e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Anwender\\Downloads\\LectorAI_SOSE24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ad96899b-029c-4b28-869e-c4f0ecb36420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming checkpoint_path, checkpoint_dir, and colormap_path are strings representing file paths\n",
    "checkpoint_path = 'dokumentenklassifikation/workspace/models/working_folder/kaggle/working/maskrcnn/MASKRCNN/checkpath_model/2024-05-24_20-57-54'\n",
    "checkpoint_dir = checkpoint_path\n",
    "colormap_path = checkpoint_path\n",
    "checkpoint_dir_path = checkpoint_path\n",
    "# Convert the paths to Path objects\n",
    "checkpoint_path_obj = Path(checkpoint_path)\n",
    "checkpoint_dir_obj = Path(checkpoint_dir)\n",
    "colormap_path_obj = Path(colormap_path)\n",
    "\n",
    "# Set a filename for the ONNX model\n",
    "onnx_file_path = 'dokumentenklassifikation/workspace/models/working_folder/kaggle/working/maskrcnn/MASKRCNN/checkpath_model/2024-05-24_20-57-54/Bilder-maskrcnn_resnet50_fpn_v2.onnx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6fee8c1-7942-4bd9-952f-3b278028a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_sz = 1024\n",
    "test_file = 'dokumentenklassifikation/MASKRCNN/test_folder/images/AD_016.jpg'  # path to test image or from frontend/backend\n",
    "\n",
    "# Open the test file\n",
    "test_img = Image.open(test_file).convert('RGB')\n",
    "\n",
    "# Convert the resized image back to PIL format\n",
    "input_img = tf.keras.preprocessing.image.img_to_array(input_img)\n",
    "\n",
    "# Calculate the scale between the source image and the resized image\n",
    "min_img_scale = min(test_img.size) / min(input_img.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b52bdd34-1a59-4766-9474-ecf96883b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the colormap JSON file in the directory\n",
    "colormap_path = list(checkpoint_dir_obj.glob('*colormap.json'))[0]\n",
    "\n",
    "# Load the JSON colormap data\n",
    "with open(colormap_path, 'r') as file:\n",
    "    colormap_json = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a dictionary        \n",
    "colormap_dict = {item['label']: item['color'] for item in colormap_json['items']}\n",
    "\n",
    "# Extract the class names from the colormap\n",
    "class_names = list(colormap_dict.keys())\n",
    "\n",
    "# Make a copy of the colormap in integer format\n",
    "int_colors = [tuple(int(c*255) for c in color) for color in colormap_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0538580-5336-4e65-8024-6a040452ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_params = {\n",
    "    \"ad_erzieher_name\":         {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"ad_erzieher_vorname\":      {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"ad_erzieher_tel\":          {\"left\": 0.2, \"bottom\": 0},\n",
    "    \"ad_erzieher_email\":        {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"schueler_name\":            {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"schueler_vorname\":         {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"schueler_klasse\":          {\"left\": 0.125, \"bottom\": 0},\n",
    "    \"ad_neue_ad_str_haus_nr\":   {\"left\": 0.275, \"bottom\": 0},\n",
    "    \"ad_neue_ad_plz\":           {\"left\": 0.25, \"bottom\": 0},\n",
    "    \"ad_neue_ad_stadt\":         {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"ad_schueler_datum\":        {\"left\": 0.2, \"bottom\": 0},\n",
    "    \"ag_auswahl_wahl_1\":        {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"ag_auswahl_wahl_2\":        {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"ag_auswahl_wahl_3\":        {\"left\": 0.15, \"bottom\": 0},\n",
    "    \"ag_schueler_datum\":        {\"left\": 0.3, \"bottom\": 0},\n",
    "    \"ad_schueler_unterschrift\": {\"left\": 0.2, \"bottom\": 0}, \n",
    "    \"ag_schueler_unterschrift\": {\"left\": 0.2, \"bottom\": 0}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "195a3597-147c-4cbc-a1ad-51c7aa4fb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_and_crop_images(pred_labels, pred_bboxes, cropping_params, test_img):\n",
    "    \"\"\"\n",
    "    Adjusts bounding boxes based on cropping parameters and crops the images.\n",
    "\n",
    "    Args:\n",
    "    pred_labels (list): List of predicted labels.\n",
    "    pred_bboxes (list): List of predicted bounding boxes.\n",
    "    cropping_params (dict): Dictionary with cropping parameters. must be adjusted in some images \n",
    "    test_img (PIL.Image): The original image to crop.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cropped images.\n",
    "    \"\"\"\n",
    "    cropped_images = []\n",
    "    \n",
    "    for label, box in zip(pred_labels, pred_bboxes):\n",
    "        box = box.astype(int).tolist()  # Convert tensor to list of integers\n",
    "        if label in cropping_params:\n",
    "            params = cropping_params[label]\n",
    "            left_adjust = params.get(\"left\", 0)\n",
    "            bottom_adjust = params.get(\"bottom\", 0)\n",
    "            \n",
    "            # Adjust the bounding box\n",
    "            box[0] += int((box[2] - box[0]) * left_adjust)\n",
    "            box[3] -= int((box[3] - box[1]) * bottom_adjust)\n",
    "        \n",
    "        cropped_image = test_img.crop((box[0], box[1], box[2], box[3]))\n",
    "        cropped_images.append(cropped_image)\n",
    "    \n",
    "    return cropped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6202bfa9-c14d-47ea-9721-a2ba01a671f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_on_image_inference(test_img, onnx_file_path, class_names, threshold=0.8):\n",
    "  \n",
    "    train_sz = 1024\n",
    "\n",
    "    # Resize the test image\n",
    "    input_img = tf.image.resize(test_img, [train_sz, train_sz])\n",
    "\n",
    "     # Convert the resized image to a tensor\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "\n",
    "    # Add a batch dimension to the input tensor\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "    # Calculate the scale between the source image and the resized image\n",
    "    min_img_scale = min(test_img.size) / min(input_img.shape[:2])    \n",
    "    # Load the model and create an InferenceSession\n",
    "    session = ort.InferenceSession(onnx_file_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "    # Transpose the input tensor to match the expected format \n",
    "    input_tensor = tf.transpose(input_tensor, perm=[0, 3, 1, 2])  \n",
    "    input_tensor = input_tensor.numpy().astype(np.float32) / 255.0\n",
    "    \n",
    "    model_output = session.run(None, {\"input\": input_tensor})\n",
    "\n",
    "    # Filter the output based on the confidence threshold\n",
    "    scores_mask = model_output[2] > threshold\n",
    "    \n",
    "    label_list = [class_names[int(idx)] for idx in model_output[1][scores_mask]]\n",
    "\n",
    "    # Scale the predicted bounding boxes\n",
    "    pred_bboxes = (model_output[0][scores_mask])*min_img_scale\n",
    "\n",
    "    # Get the class names for the predicted label indices\n",
    "    pred_labels = [class_names[int(idx)] for idx in model_output[1][scores_mask]]\n",
    "\n",
    "    # Extract the confidence scores\n",
    "    pred_scores = model_output[2]\n",
    "    \n",
    "    colors = [int_colors[class_names.index(i)] for i in label_list]\n",
    "\n",
    "    # Adjust and crop the images based on the bounding boxes and cropping parameters\n",
    "    cropped_images = adjust_and_crop_images(pred_labels, pred_bboxes, cropping_params, test_img)\n",
    "\n",
    "    # Combine the bounding boxes, cropped images, labels, and confidence scores\n",
    "    results = []\n",
    "    for label, box, image, score in zip(pred_labels, pred_bboxes, cropped_images, pred_scores):\n",
    "        results.append((label, box, image, score.item()))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2db84683-237e-4e2e-a436-5c6f1d924537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = prediction_on_image_inference(test_img, onnx_file_path, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dd7725fc-2a20-4b54-b512-43137739ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: schueler, Box: [201.02843 554.6504  767.4948  680.48376], Confidence: 0.9993707537651062\n",
      "Label: ad_neue_ad, Box: [100.22949 692.0265  787.785   814.8419 ], Confidence: 0.9988994598388672\n",
      "Label: ad_erzieher_name, Box: [233.00212 416.80637 752.3979  446.9802 ], Confidence: 0.9987454414367676\n",
      "Label: ad_erzieher_email, Box: [224.93173 508.7705  758.86176 538.91223], Confidence: 0.9985170960426331\n",
      "Label: ad_neue_ad_stadt, Box: [236.77263 780.24884 761.2298  806.95337], Confidence: 0.9983243346214294\n",
      "Label: ad_erzieher, Box: [158.95529 388.83246 781.355   542.11743], Confidence: 0.9981778860092163\n",
      "Label: schueler_klasse, Box: [231.70908 643.9477  757.90076 672.05695], Confidence: 0.9980339407920837\n",
      "Label: ad_erzieher_vorname, Box: [202.30026 452.39795 753.15936 476.5314 ], Confidence: 0.997865617275238\n",
      "Label: ad_neue_ad_str_haus_nr, Box: [115.968575 718.42426  762.1873   747.2737  ], Confidence: 0.9971975088119507\n",
      "Label: ad_erzieher_tel, Box: [171.08679 480.6874  757.22626 506.32352], Confidence: 0.9967193007469177\n",
      "Label: ad_unterschrift, Box: [196.1414  899.97784 946.3672  958.18805], Confidence: 0.9961513876914978\n",
      "Label: ad_neue_ad_plz, Box: [199.21346 747.8328  757.98584 776.76013], Confidence: 0.9939047694206238\n",
      "Label: adress_aend, Box: [188.18013 201.92673 797.65753 245.05849], Confidence: 0.9936891794204712\n",
      "Label: schueler_vorname, Box: [214.16429 615.9339  755.2983  641.2318 ], Confidence: 0.9925656318664551\n",
      "Label: schueler_name, Box: [232.65306 585.8276  759.06134 610.9944 ], Confidence: 0.990349292755127\n",
      "Label: ad_schueler_datum, Box: [716.3893  900.26685 941.35565 933.675  ], Confidence: 0.9887058138847351\n",
      "Label: ad_schueler_unterschrift, Box: [205.84557 905.4306  601.70264 956.7892 ], Confidence: 0.9878163933753967\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "for label, box, image, score in results:\n",
    "    print(f\"Label: {label}, Box: {box}, Confidence: {score}\")\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e80800c3-84eb-41dd-a86a-6afeac688a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('schueler',\n",
       "  array([161.90598, 553.10815, 830.1276 , 682.4923 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=669x129>,\n",
       "  0.9988780617713928),\n",
       " ('ad_erzieher',\n",
       "  array([109.01843, 374.5692 , 854.89325, 545.6848 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=745x171>,\n",
       "  0.9987834095954895),\n",
       " ('ad_neue_ad',\n",
       "  array([ 84.6607 , 687.52814, 816.2743 , 834.4951 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=732x147>,\n",
       "  0.9987322688102722),\n",
       " ('ad_erzieher_name',\n",
       "  array([204.4028 , 405.68066, 821.3466 , 446.98877], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=617x41>,\n",
       "  0.9982062578201294),\n",
       " ('adress_aend',\n",
       "  array([326.71072, 183.33566, 696.41327, 247.70406], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=370x64>,\n",
       "  0.9967946410179138),\n",
       " ('ad_schueler_unterschrift',\n",
       "  array([184.2755 , 908.95514, 639.8978 , 957.21454], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=455x49>,\n",
       "  0.9966655373573303),\n",
       " ('ad_erzieher_email',\n",
       "  array([214.20378, 499.49283, 801.69714, 541.94275], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=587x42>,\n",
       "  0.9962041974067688),\n",
       " ('ad_erzieher_tel',\n",
       "  array([174.92049, 470.0978 , 797.42883, 508.49686], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=623x38>,\n",
       "  0.9959580302238464),\n",
       " ('ad_unterschrift',\n",
       "  array([148.15105, 899.9558 , 995.64246, 973.18945], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=847x74>,\n",
       "  0.9939420819282532),\n",
       " ('schueler_name',\n",
       "  array([213.59761, 565.96326, 806.4873 , 613.76263], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=593x48>,\n",
       "  0.9934533834457397),\n",
       " ('schueler_klasse',\n",
       "  array([203.5555, 636.9694, 808.9319, 679.1392], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=605x43>,\n",
       "  0.9885525703430176),\n",
       " ('ad_neue_ad_stadt',\n",
       "  array([215.49779, 775.1737 , 791.3095 , 821.47217], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=576x46>,\n",
       "  0.9881858825683594),\n",
       " ('ad_neue_ad_str_haus_nr',\n",
       "  array([102.98031, 711.1895 , 794.4231 , 745.1776 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=692x34>,\n",
       "  0.985664963722229),\n",
       " ('ad_schueler_datum',\n",
       "  array([717.6222 , 906.99896, 963.7437 , 943.2563 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=246x37>,\n",
       "  0.9825907349586487),\n",
       " ('ad_erzieher_vorname',\n",
       "  array([198.88214, 440.98218, 813.89825, 477.5579 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=615x37>,\n",
       "  0.9703381061553955),\n",
       " ('schueler_vorname',\n",
       "  array([197.0711 , 607.69086, 802.5486 , 650.9683 ], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=605x43>,\n",
       "  0.9412469863891602),\n",
       " ('ad_schueler_datum',\n",
       "  array([729.6212, 922.0353, 978.3562, 956.7908], dtype=float32),\n",
       "  <PIL.Image.Image image mode=RGB size=249x34>,\n",
       "  0.9218788743019104)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a46a5-429c-499a-b9a8-e100fe20b12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
