{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe864d5b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40281f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, default_data_collator\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32228c",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad01d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "EPOCHS = 25\n",
    "MODEL_NAME = \"small-stage1\"\n",
    "AUGMENTATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4eb795",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_source_model_name = \"microsoft/trocr-small-stage1\"\n",
    "base_path = \"models/trocr/\"\n",
    "save_model_name = os.path.join(base_path, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/transfer_dataset/'\n",
    "train_dataset_path = os.path.join(dataset_path, 'train')\n",
    "val_dataset_path = os.path.join(dataset_path, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632d040",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce0323-d185-4e6f-b9ef-04c218f7c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_list = os.listdir(train_dataset_path)\n",
    "val_df_list = os.listdir(val_dataset_path)\n",
    "\n",
    "train_df_jpg_list = [train_df_list[i] for i in range(len(train_df_list)) if train_df_list[i].endswith('.jpg')]\n",
    "val_df_jpg_list = [val_df_list[i] for i in range(len(val_df_list)) if val_df_list[i].endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc504698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=['file_name', 'text'])\n",
    "val_df = pd.DataFrame(columns=['file_name', 'text'])\n",
    "\n",
    "for i in range(len(train_df_jpg_list)):\n",
    "    text_file = f\"{train_df_jpg_list[i].split('.')[0]}.txt\"\n",
    "    with open(os.path.join(dataset_path, 'train', text_file), 'r') as f:\n",
    "        text = f.read()\n",
    "    train_df.loc[i] = {'file_name': train_df_jpg_list[i], 'text': text.replace('|', ' ')}\n",
    "\n",
    "for i in range(len(val_df_jpg_list)):\n",
    "    text_file = f\"{val_df_jpg_list[i].split('.')[0]}.txt\"\n",
    "    with open(os.path.join(dataset_path, 'val', text_file), 'r') as f:\n",
    "        text = f.read()\n",
    "    val_df.loc[i] = {'file_name': val_df_jpg_list[i], 'text': text.replace('|', ' ')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84351e98",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463236f-01e8-4c49-b5ff-5fee06f836f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128, augment=False, target_size=(1024, 128)):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "\n",
    "\n",
    "        if augment:\n",
    "            self.augment_transforms = transforms.Compose([\n",
    "                transforms.RandomRotation(2),  \n",
    "                transforms.ColorJitter(brightness=0.25, contrast=0.25),  # Randomly change brightness, contrast, etc.\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = str(self.df['text'][idx])\n",
    "        \n",
    "        image_path = os.path.join(self.root_dir, file_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.augment:\n",
    "            image = self.augment_transforms(image)\n",
    "        \n",
    "        \n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        labels = self.processor.tokenizer(text, padding=\"max_length\", max_length=self.max_target_length, truncation=True).input_ids\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a3e66",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf03fe-c8f0-4b05-9078-9cfdaf39ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(open_source_model_name, )\n",
    "train_dataset = Dataset(root_dir=train_dataset_path,\n",
    "                           df=train_df,\n",
    "                           processor=processor, augment=AUGMENTATION)\n",
    "eval_dataset = Dataset(root_dir=val_dataset_path,\n",
    "                           df=val_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781ad02-a153-4171-8099-cc74fa70eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea5799",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot Example Image\n",
    "plt.imshow(train_dataset[0][\"pixel_values\"].permute(1, 2, 0))\n",
    "print(\"Image shape:\", train_dataset[0][\"pixel_values\"].shape)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0ef74",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d05ce1-df74-451b-8e15-41c36db24708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(open_source_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35b9a9-7c30-4f66-bd2e-83996012c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_gram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e076a5a-1084-4bae-b6bf-82ea816fd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(  \n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    fp16=False, \n",
    "    output_dir=save_model_name,\n",
    "    logging_steps=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"levenshtein\",  \n",
    "    greater_is_better=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341d9cc-052d-4371-b952-896508aaf00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f98ff8-4310-46e3-a018-73ab423f2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    sum_leven = 0\n",
    "    for label, pred in zip(label_str, pred_str):\n",
    "        sum_leven += distance(label, pred)\n",
    "    levenshtein = sum_leven / len(label_str)\n",
    "    \n",
    "    return {\"levenshtein\": levenshtein}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022190b8-b1c5-4764-826e-6a20a55d29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.image_processor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28250815",
   "metadata": {},
   "source": [
    "### Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e464e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have completed training and your trainer object is named 'trainer'\n",
    "# Extracting metrics from the log history\n",
    "log_history = trainer.state.log_history\n",
    "epochs = []\n",
    "train_epochs = []\n",
    "levenshtein_distances = []\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "for entry in log_history:\n",
    "    if 'eval_loss' in entry: \n",
    "        epochs.append(entry['epoch'])\n",
    "        levenshtein_distances.append(entry['eval_levenshtein'])\n",
    "       \n",
    "        eval_losses.append(entry[\"eval_loss\"])\n",
    "    if 'loss' in entry:\n",
    "        train_losses.append(entry[\"loss\"])\n",
    "        train_epochs.append(entry['epoch'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, levenshtein_distances, marker='.', linestyle='-', color='g')\n",
    "plt.plot(train_epochs, train_losses, marker='.', linestyle='-', color='r')\n",
    "plt.plot(epochs, eval_losses, marker='.', linestyle='-', color='b')\n",
    "plt.legend([\"Levenshtein Distance\", \"Training Loss\", \"Eval Loss\"])\n",
    "plt.title(f'Model: {os.path.basename(save_model_name)} Levenshtein: {round(min(levenshtein_distances),2)} Loss: {round(min(train_losses),2)} Eval Loss: {round(min(eval_losses),2)} ')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.ylabel([\"Levenshtein Distance\", \"Training Loss\", \"Eval Loss\"])\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{save_model_name}/results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246dc8b",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(save_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f70516",
   "metadata": {},
   "source": [
    "### Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba333ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(open_source_model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94174df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted; True\")\n",
    "for i, eval in enumerate(eval_dataset):\n",
    "    pixel_values = eval['pixel_values'].unsqueeze(0)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    real_text = val_df['text'][i]\n",
    "    print(generated_text, real_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf.gpusupenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
